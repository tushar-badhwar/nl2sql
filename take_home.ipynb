{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf393066",
   "metadata": {},
   "source": [
    "#### Install Anthropic's SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298a5136",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install anthropic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4fa951",
   "metadata": {},
   "source": [
    "#### Use ipython store to manage your Anthropic API_KEY, MODEL_NAME\n",
    "- You can get your api key from your console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "ef018029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'API_KEY' (str)\n",
      "Stored 'MODEL_NAME' (str)\n"
     ]
    }
   ],
   "source": [
    "API_KEY=\"your_api_key\"\n",
    "MODEL_NAME=\"claude-3-7-sonnet-20250219\"\n",
    "%store API_KEY\n",
    "%store MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5c5772",
   "metadata": {},
   "source": [
    "##### Additional Python Store functionalities\n",
    "- Should you need to retrieve these keys in another python notebook you can uncomment the following code and run it in your notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dd76ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store -r API_KEY\n",
    "#%store -r MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96732ef1",
   "metadata": {},
   "source": [
    "#### Importing the data and other related dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c4c66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "db_path = r\"C:\\Users\\badhw\\Downloads\\nba.sqlite\" # replace with your db_path\n",
    "json_path=r\"C:\\Users\\badhw\\Downloads\\ground_truth_data.json\" # replace with your json_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "5774e36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading ground truth data from the json file\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    ground_truth_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "2201601d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'natural_language': 'How many teams are in the NBA?',\n",
       "  'sql': 'SELECT COUNT(*) as team_count FROM team LIMIT 1',\n",
       "  'type': 'counting'},\n",
       " {'natural_language': 'What are the 5 oldest teams in the NBA?',\n",
       "  'sql': 'SELECT full_name FROM team ORDER BY year_founded ASC LIMIT 5',\n",
       "  'type': 'ranking'},\n",
       " {'natural_language': 'List all teams from California',\n",
       "  'sql': \"SELECT full_name FROM team WHERE state = 'California'\",\n",
       "  'type': 'filtering'},\n",
       " {'natural_language': \"What's the total number of games played?\",\n",
       "  'sql': 'SELECT COUNT(DISTINCT game_id) as total_games FROM game LIMIT 1',\n",
       "  'type': 'counting'},\n",
       " {'natural_language': \"What's the highest scoring game?\",\n",
       "  'sql': 'SELECT g.pts_home + g.pts_away as total_points FROM game g ORDER BY total_points DESC LIMIT 1',\n",
       "  'type': 'ranking'}]"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if data is loaded by printing top 5 rows.\n",
    "ground_truth_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "ab70bac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking number of ground truth data entries\n",
    "len(ground_truth_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "b0b6c4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'counting': 19,\n",
       " 'ranking': 30,\n",
       " 'filtering': 13,\n",
       " 'aggregation': 25,\n",
       " 'detail': 7,\n",
       " 'comparison': 3,\n",
       " 'history': 1}"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract unique types of queries and their value counts\n",
    "query_type_counts = {}\n",
    "for i in ground_truth_data:\n",
    "    query_type = i.get(\"type\")\n",
    "    query_type_counts[query_type] = query_type_counts.get(query_type, 0) + 1\n",
    "\n",
    "query_type_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dffef8d",
   "metadata": {},
   "source": [
    "#### Setting up the API call to Claude\n",
    "- We will use this function to interact with Claude\n",
    "- For NL-> SQL tasks, Claude 3.7 Sonnet is one Anthropic's best performing models.\n",
    "- However, should you want to change the model, you can always refer to our model codes and change the MODEL_NAME variable at the top of this notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "0fc772a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "client = anthropic.Anthropic(api_key=API_KEY)\n",
    "def api_call(prompt:str):\n",
    "    message=client.messages.create(\n",
    "        model=MODEL_NAME, # using Claud 3.7\n",
    "        max_tokens=2000, # good range for NL->SQL tasks\n",
    "        temperature=0.0, # more deterministic\n",
    "        messages=[{\"role\":\"user\",\"content\":prompt}] # standard claude api message template always starts with user, can be continued with a user-assistant-user-... format\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5223afc0",
   "metadata": {},
   "source": [
    "#### Function to run the ground truth queries on the DB\n",
    "- Arguments: The list of dictionaries which has the ground truth question-SQL query pairs\n",
    "- Returns: None\n",
    "- What it does: The function runs the ground truth SQL on the DB and then adds the ground truth result of the SQL into the ground truth json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "ba6f50f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ground_truth_query(input_data,db_path):\n",
    "\n",
    "    for sample in input_data:\n",
    "        sql = sample[\"sql\"]\n",
    "        try:\n",
    "            conn= sqlite3.connect(db_path)\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(sql)\n",
    "            results = cursor.fetchall()\n",
    "            conn.close()\n",
    "            \n",
    "            # below code flattens the SQL result into a list depending upon the query output\n",
    "            if len(results) > 0 and len(results[0]) == 1:\n",
    "                flat = [row[0] for row in results]\n",
    "            else:\n",
    "                flat = results\n",
    "            sample[\"ground_truth_result\"]=flat\n",
    "        except sqlite3.Error as e:\n",
    "            sample[\"ground_truth_result\"]=None\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae710cb",
   "metadata": {},
   "source": [
    "#### Running the ground truth queries on the DB\n",
    "- This can be run just once, and our ground truth SQL output will be populated in the ground turth jason along with the ground truth questions and SQL\n",
    "- This should take ~7 seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "49e0b701",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ground_truth_query(ground_truth_data,db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95008699",
   "metadata": {},
   "source": [
    "##### Checking the ground truth data for ground truth SQL outputs\n",
    "- You can see the ground truth results have been added after running the cell below, (top 5 only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "9ae3b9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'natural_language': 'How many teams are in the NBA?',\n",
       "  'sql': 'SELECT COUNT(*) as team_count FROM team LIMIT 1',\n",
       "  'type': 'counting',\n",
       "  'ground_truth_result': [30]},\n",
       " {'natural_language': 'What are the 5 oldest teams in the NBA?',\n",
       "  'sql': 'SELECT full_name FROM team ORDER BY year_founded ASC LIMIT 5',\n",
       "  'type': 'ranking',\n",
       "  'ground_truth_result': ['Boston Celtics',\n",
       "   'Golden State Warriors',\n",
       "   'New York Knicks',\n",
       "   'Los Angeles Lakers',\n",
       "   'Sacramento Kings']},\n",
       " {'natural_language': 'List all teams from California',\n",
       "  'sql': \"SELECT full_name FROM team WHERE state = 'California'\",\n",
       "  'type': 'filtering',\n",
       "  'ground_truth_result': ['Golden State Warriors',\n",
       "   'Los Angeles Clippers',\n",
       "   'Los Angeles Lakers',\n",
       "   'Sacramento Kings']},\n",
       " {'natural_language': \"What's the total number of games played?\",\n",
       "  'sql': 'SELECT COUNT(DISTINCT game_id) as total_games FROM game LIMIT 1',\n",
       "  'type': 'counting',\n",
       "  'ground_truth_result': [65642]},\n",
       " {'natural_language': \"What's the highest scoring game?\",\n",
       "  'sql': 'SELECT g.pts_home + g.pts_away as total_points FROM game g ORDER BY total_points DESC LIMIT 1',\n",
       "  'type': 'ranking',\n",
       "  'ground_truth_result': [374.0]}]"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fcecdc",
   "metadata": {},
   "source": [
    "#### Function to generate SQL from Claude\n",
    "- Arguments: list containing ground truth question-SQL pairs and query results, api_call function to claude\n",
    "- Returns: list of sql queries in the same order as questions in the ground truth.\n",
    "- Based on Claude's outut, there is also a helper function to extract sql from the output if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "483b6728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_raw_sql(response): # helper function to get raw SQL from claudes output\n",
    "    match = re.search(r\"```sql\\s+(.*?)```\", response, re.DOTALL | re.IGNORECASE) \n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def generate_sql_from_claude(input_data,prompt_template):\n",
    "\n",
    "    claude_generated_sql_list=[]\n",
    "    \n",
    "    for i in input_data:\n",
    "        question=i['natural_language'] # extracting the natural language question from the ground truth data\n",
    "        query_type=i['type'] # extract query type\n",
    "        prompt_vars = {\"question\": question,\"query_type\": query_type} # this is used so that it works for both the original prompt and prompt engineering\n",
    "        prompt=prompt_template.format(**prompt_vars) # injecting our question, query_type(depending on the prompt) into the prompt template\n",
    "        claude_sql=api_call(prompt) #generating sql using claude\n",
    "        claude_generated_sql_list.append(claude_sql)\n",
    "    \n",
    "    return claude_generated_sql_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e2a28b",
   "metadata": {},
   "source": [
    "#### Run the below cell to get a list of SQL queries generated by Claude\n",
    "- First run the cell with the prompt, which is the original prompt in your Email to me\n",
    "- Then run the cells below to generate SQL using Claude, that will print Claude's response to the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "364976fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_prompt=\"\"\"\n",
    "Convert this question to SQL:\n",
    "{question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e42a67",
   "metadata": {},
   "source": [
    "**Important Note**\n",
    "- If you'd like to generate the SQL using claude on a 20% random sample of ground truth data, then uncomment the cell below and then run the generate_sql_from_claude function. This will decrease the latency and in production environments - possible rate limiting. **In this case, we make sure to use the ground_truth_sample dataset everywhere else.**\n",
    "- If you'd like to generate SQL on the entire dataset then run the generate_sql_from_claude function directly. The code provided here does that and will take ~5-6 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404a1340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# random.seed(42)\n",
    "# ground_truth_sample = random.sample(ground_truth_data,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "aadb2d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "claude_response=generate_sql_from_claude(ground_truth_data,original_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57af9963",
   "metadata": {},
   "source": [
    "- Printing the 5 questions and Claude generated SQL for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "dcbbc53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################ \n",
      " question How many teams are in the NBA?:\n",
      " ```sql\n",
      "SELECT COUNT(*) AS total_teams\n",
      "FROM teams\n",
      "WHERE league = 'NBA';\n",
      "```\n",
      "\n",
      "This SQL query counts the number of teams in the NBA by:\n",
      "1. Selecting the COUNT of all rows\n",
      "2. From the teams table\n",
      "3. Where the league is 'NBA'\n",
      "4. Returning the result as \"total_teams\"\n",
      "\n",
      "Note: This assumes there is a \"teams\" table with a \"league\" column that contains the value 'NBA' for NBA teams. If your database has a different structure, the query would need to be adjusted accordingly. \n",
      "\n",
      "############################################################ \n",
      " question What are the 5 oldest teams in the NBA?:\n",
      " ```sql\n",
      "SELECT team_name, year_founded\n",
      "FROM teams\n",
      "WHERE league = 'NBA'\n",
      "ORDER BY year_founded ASC\n",
      "LIMIT 5;\n",
      "```\n",
      "\n",
      "This SQL query:\n",
      "1. Selects the team name and founding year from the teams table\n",
      "2. Filters for only NBA teams\n",
      "3. Orders the results by founding year in ascending order (oldest first)\n",
      "4. Limits the results to the top 5 teams \n",
      "\n",
      "############################################################ \n",
      " question List all teams from California:\n",
      " ```sql\n",
      "SELECT *\n",
      "FROM teams\n",
      "WHERE state = 'California';\n",
      "``` \n",
      "\n",
      "############################################################ \n",
      " question What's the total number of games played?:\n",
      " ```sql\n",
      "SELECT COUNT(*) AS total_games_played\n",
      "FROM games;\n",
      "```\n",
      "\n",
      "This SQL query counts the total number of rows in the \"games\" table, assuming each row represents a single game played. The result is labeled as \"total_games_played\" for clarity. \n",
      "\n",
      "############################################################ \n",
      " question What's the highest scoring game?:\n",
      " ```sql\n",
      "SELECT TOP 1 *\n",
      "FROM games\n",
      "ORDER BY score DESC\n",
      "```\n",
      "\n",
      "This SQL query selects the game with the highest score by:\n",
      "1. Using `SELECT TOP 1 *` to retrieve all columns for just one record\n",
      "2. Ordering all games by their score in descending order (`ORDER BY score DESC`)\n",
      "3. Taking only the first result (the one with the highest score)\n",
      "\n",
      "Note: If you're using MySQL or SQLite instead of SQL Server, you might need to use `LIMIT 1` instead of `TOP 1`:\n",
      "```sql\n",
      "SELECT *\n",
      "FROM games\n",
      "ORDER BY score DESC\n",
      "LIMIT 1\n",
      "``` \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,j in zip(ground_truth_data[:5],claude_response[:5]):\n",
    "    print(\"#\"*60,f\"\\n question {i['natural_language']}:\\n\",j,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db71a5b",
   "metadata": {},
   "source": [
    "#### We can see that the SQL comes with other results/assumptions Claude generates\n",
    "- We can run the below cells to just get the raw SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "09409618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################ \n",
      " question: How many teams are in the NBA?\n",
      " SQL:\n",
      " SELECT COUNT(*) AS total_teams\n",
      "FROM teams\n",
      "WHERE league = 'NBA'; \n",
      "\n",
      "############################################################ \n",
      " question: What are the 5 oldest teams in the NBA?\n",
      " SQL:\n",
      " SELECT team_name, year_founded\n",
      "FROM teams\n",
      "WHERE league = 'NBA'\n",
      "ORDER BY year_founded ASC\n",
      "LIMIT 5; \n",
      "\n",
      "############################################################ \n",
      " question: List all teams from California\n",
      " SQL:\n",
      " SELECT *\n",
      "FROM teams\n",
      "WHERE state = 'California'; \n",
      "\n",
      "############################################################ \n",
      " question: What's the total number of games played?\n",
      " SQL:\n",
      " SELECT COUNT(*) AS total_games_played\n",
      "FROM games; \n",
      "\n",
      "############################################################ \n",
      " question: What's the highest scoring game?\n",
      " SQL:\n",
      " SELECT TOP 1 *\n",
      "FROM games\n",
      "ORDER BY score DESC \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def responses_to_sql_list(responses):\n",
    "    return [extract_raw_sql(r) for r in responses]\n",
    "\n",
    "claude_sql_raw=responses_to_sql_list(claude_response)\n",
    "for i,j in zip(ground_truth_data[:5],claude_sql_raw[:5]):\n",
    "    print(\"#\"*60,f\"\\n question: {i['natural_language']}\\n SQL:\\n\",j,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eb60de",
   "metadata": {},
   "source": [
    "#### Function to run Claude generated SQL on the DB (this is quite similar to the earlier function for ground truth)\n",
    "- The function below: run_claude_query is called in the final evaluation function (evaluate_sql_accuracy) where the SQL query generated by Claude for every question is run on the database and compared with the ground truth output.\n",
    "- If the running the SQL on the DB returns an Error, the result is recorded as None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "da8ef364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_claude_query(sql,db_path):\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(sql)\n",
    "        results = cursor.fetchall()\n",
    "        conn.close()\n",
    "        # flatten query results    \n",
    "        if len(results) > 0 and len(results[0]) == 1:\n",
    "            flat = [row[0] for row in results]\n",
    "        else:\n",
    "            flat = results\n",
    "        return flat\n",
    "    except sqlite3.Error as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2887e8ad",
   "metadata": {},
   "source": [
    "#### Function to evaluate the results & accuracy of the Claude generated SQL with the ground truth\n",
    "- Arguments: Ground truth data, Claude generated sql, db_path\n",
    "- Returns: Overall accuracy, Dictionary with individual results for each question\n",
    "#### Notes about this evaluation\n",
    "- Quantitative evaluation (Semantic query equivalence): We compare the results of the running Claude's queries on the DB with the ground truth SQL results to evaluate SQL semantic similarity.\n",
    "- Two different queries could product the same result, which is why we're comparing the results\n",
    "- **Accuracy** =  $\\frac{\\text{number of events where Claude's SQL output equals ground-truth result}}{\\text{total number of questions asked}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "2881ca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sql_accuracy(input_data,claude_generated_sqls,db_path):\n",
    "\n",
    "    detailed_results= [] # list of di tionaries storing details mentioned below in the function\n",
    "    correct = 0\n",
    "\n",
    "    for index, data in enumerate(input_data):\n",
    "        gt_result = data[\"ground_truth_result\"] \n",
    "        claude_sql = claude_generated_sqls[index]\n",
    "        claude_sql_result = run_claude_query(claude_sql,db_path) # run claude generated sql\n",
    "\n",
    "        is_match = claude_sql_result is not None and claude_sql_result == gt_result # checks results for ground truth and query matching, and for any errors.\n",
    "        if is_match:\n",
    "            correct +=1\n",
    "        \n",
    "        detailed_results.append(\n",
    "            {\n",
    "                \"question\": data[\"natural_language\"],\n",
    "                \"ground_truth_sql\": data[\"sql\"],\n",
    "                \"gorund_truth_query_type\": data[\"type\"],\n",
    "                \"claude_sql\": claude_sql,\n",
    "                \"sql_semantic_match\": is_match,\n",
    "                \"ground_truth\": gt_result,\n",
    "                \"claude_sql_result\": claude_sql_result,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    accuracy = correct/len(input_data) if data else 0.0\n",
    "    return {\"accuracy\": accuracy, \"detailed_results\": detailed_results}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d145dc3",
   "metadata": {},
   "source": [
    "#### Run below cells to evaluate the original prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "5fdecf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=evaluate_sql_accuracy(ground_truth_data,claude_sql_raw,db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78b342d",
   "metadata": {},
   "source": [
    "#### Top 2 results of the evaluation, this shows the question, ground truth sql and semantic match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "9e205ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"question\": \"How many teams are in the NBA?\",\n",
      "  \"ground_truth_sql\": \"SELECT COUNT(*) as team_count FROM team LIMIT 1\",\n",
      "  \"gorund_truth_query_type\": \"counting\",\n",
      "  \"claude_sql\": \"SELECT COUNT(*) AS total_teams\\nFROM teams\\nWHERE league = 'NBA';\",\n",
      "  \"sql_semantic_match\": false,\n",
      "  \"ground_truth\": [\n",
      "    30\n",
      "  ],\n",
      "  \"claude_sql_result\": null\n",
      "}\n",
      "{\n",
      "  \"question\": \"What are the 5 oldest teams in the NBA?\",\n",
      "  \"ground_truth_sql\": \"SELECT full_name FROM team ORDER BY year_founded ASC LIMIT 5\",\n",
      "  \"gorund_truth_query_type\": \"ranking\",\n",
      "  \"claude_sql\": \"SELECT team_name, year_founded\\nFROM teams\\nWHERE league = 'NBA'\\nORDER BY year_founded ASC\\nLIMIT 5;\",\n",
      "  \"sql_semantic_match\": false,\n",
      "  \"ground_truth\": [\n",
      "    \"Boston Celtics\",\n",
      "    \"Golden State Warriors\",\n",
      "    \"New York Knicks\",\n",
      "    \"Los Angeles Lakers\",\n",
      "    \"Sacramento Kings\"\n",
      "  ],\n",
      "  \"claude_sql_result\": null\n",
      "}\n",
      "Overall accuracy: 0.0 %\n"
     ]
    }
   ],
   "source": [
    "for i in results['detailed_results'][:2]:\n",
    "    print(json.dumps(i, indent=2))\n",
    "print(\"Overall accuracy:\", round(results['accuracy']*100,2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e41c32a",
   "metadata": {},
   "source": [
    "#### Evaluating the original prompt setup:\n",
    "\n",
    "- The current prompt is **zero-shot**, which means that Claude recieved a request without any context or examples to refer at all. Before extracting the raw SQL from Claude's responses, we could see that it had made various assumptions about the DB schema, column names and mentioned those assumptions in its response. Those are placeholder tables and will usually not be the same as the actual schema.\n",
    "- While we did run a result equivalence function here. It had a pre-determined outcome of very little to 0 accuracy, since Claude just generated queries which were based on assumptions\n",
    "- Qualitatively, we can see that the prompt requests are going through to Claude and it is generating queries which it thinks are the best fit to the question and clearly mentions in some of the responses that you might need to change table/column names appropriately.\n",
    "- Claude does well for zero shot prompts which might request factual/known information that it is already trained on. However, for mor =e specific requests it needs additional context to do well.\n",
    "- We can use prompt engineering to produce better accuracy for NL-> SQL for your analytics team"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4433e3",
   "metadata": {},
   "source": [
    "# Improving accuracy by prompt engineering\n",
    "- We will start with try various techniques to improve the accuracy of the queries we get.\n",
    "- Since we will be testing out various prompts, we will only use a **~10% random sample** of the ground_truth data to decrease latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "94c362b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "ground_truth_sample = random.sample(ground_truth_data,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90f6761",
   "metadata": {},
   "source": [
    "#### Function to run the entire setup we have done so far, with new prompts, in a single click\n",
    "- For operational ease, you can simply run the prompt_engineering_result function in the cell below, which takes the argument of the engineered prompt and then prints the results of the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "0e55c00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_engineering_result(prompt,data):\n",
    "    \n",
    "    claude_sql=generate_sql_from_claude(data,prompt) # Claude generates SQL for each question\n",
    "    claude_sql_raw=responses_to_sql_list(claude_sql)\n",
    "    results=evaluate_sql_accuracy(data,claude_sql_raw,db_path) # Claude generated SQL is run on the DB and the final accuracy is returned\n",
    "    for i in results['detailed_results'][:2]:\n",
    "        print(json.dumps(i, indent=2))\n",
    "        print(\"Overall accuracy:\", round(results['accuracy']*100,2), \"%\")\n",
    "    return results,claude_sql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845826fc",
   "metadata": {},
   "source": [
    "#### Recommendations for improving accuracy:\n",
    "\n",
    "**General recommendations**\n",
    "- Claude is trained to recognize XML tags, hence a good recommendation is to wrap specific instructions within prompts in XML tags.<br><br>\n",
    "- Prompts can be more specific by chaining various sub prompts which provide additional context to claude. We have demonstrated this below<br><br>\n",
    "- Claude can also be asked to think before it responds, this refers to **precognition** and can be a good approach to course correct Claude's responses.<br><br>\n",
    "- Giving Claude an out: Claude can also be asked to not hallucinate or refer to external resources, which is very important in NL->SQL tasks, this can be done by giving system level instructions.<br><br>\n",
    "- Few shot prompting: Claude will be more accurate if provided with some examples.<br><br>\n",
    "- Make sure the prompts are not too verbose, we can find creative ways like defining helper functions to make Claude think a certain way.\n",
    "\n",
    "**Specific recommendations**\n",
    "\n",
    "Each of these points will be sub prompts chained together, which will then be passed to Claude.\n",
    "- system_level_prompt: \n",
    "    - Incase Claude lacks enough context to generate SQL, we can give it a system level prompt asking it to not halllucinate. <br><br>\n",
    "    - We see that in zero shot prompting Claude generates queries which have WHERE LEAGUE='NBA'. We can add a system instruction saying that the DB is only for NBA by default. <br><br>\n",
    "    - We also define a helper function which Claude can call, to get details about the schema should it not have enough context from the few shot schema\n",
    "\n",
    "- query_type:\n",
    "    - We can add <query_type>{QUERY_TYPE}</query_type> variable. This will give claude context about the query type. <br><br>\n",
    "    - Earlier, we saw that for ranking queries say Top 5 oldest teams, Claude SQL output had Team Name, Year Founded in its output. To avoid this, we can give descriptions for each query type. Example: For <query_type>ranking</query_type> please make a query that outputs names of the entities to be ranked, and nothing else. So on for other query_type that are not obvious, like aggregation. <br><br>\n",
    "\n",
    "- db_schema_context:\n",
    "    - Tables to use:\n",
    "        - Earlier, we saw that Claude hallucinated table names, with our system prompt, it will not. And so it is important to provide a most commonly used table names and their descriptions. These will also be wrapped in XML tags.<br><br>\n",
    "    - Columns to use: \n",
    "        - Similarly, we can define Primary, Foreign keys and their descriptions, this will be important to get accurate column names in generated SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10163cb5",
   "metadata": {},
   "source": [
    "#### You can run below cells to get columns from tables and get insights on the actual DB schema (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "7443856b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in database: ['game', 'game_summary', 'other_stats', 'officials', 'inactive_players', 'game_info', 'line_score', 'player', 'team', 'common_player_info', 'team_details', 'team_history', 'draft_combine_stats', 'draft_history', 'team_info_common']\n"
     ]
    }
   ],
   "source": [
    "tables_query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "tables = run_claude_query(tables_query,db_path)\n",
    "print(\"Tables in database:\", tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "6a143e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns_for_table(table_name, db_path):\n",
    "    query = f\"PRAGMA table_info({table_name});\"\n",
    "    results = run_claude_query(query, db_path)\n",
    "    return [(col[1], col[2]) for col in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "e8e7c364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Table: game\n",
      "   - season_id: TEXT\n",
      "   - team_id_home: TEXT\n",
      "   - team_abbreviation_home: TEXT\n",
      "   - team_name_home: TEXT\n",
      "   - game_id: TEXT\n",
      "   - game_date: TIMESTAMP\n",
      "   - matchup_home: TEXT\n",
      "   - wl_home: TEXT\n",
      "   - min: INTEGER\n",
      "   - fgm_home: REAL\n",
      "   - fga_home: REAL\n",
      "   - fg_pct_home: REAL\n",
      "   - fg3m_home: REAL\n",
      "   - fg3a_home: REAL\n",
      "   - fg3_pct_home: REAL\n",
      "   - ftm_home: REAL\n",
      "   - fta_home: REAL\n",
      "   - ft_pct_home: REAL\n",
      "   - oreb_home: REAL\n",
      "   - dreb_home: REAL\n",
      "   - reb_home: REAL\n",
      "   - ast_home: REAL\n",
      "   - stl_home: REAL\n",
      "   - blk_home: REAL\n",
      "   - tov_home: REAL\n",
      "   - pf_home: REAL\n",
      "   - pts_home: REAL\n",
      "   - plus_minus_home: INTEGER\n",
      "   - video_available_home: INTEGER\n",
      "   - team_id_away: TEXT\n",
      "   - team_abbreviation_away: TEXT\n",
      "   - team_name_away: TEXT\n",
      "   - matchup_away: TEXT\n",
      "   - wl_away: TEXT\n",
      "   - fgm_away: REAL\n",
      "   - fga_away: REAL\n",
      "   - fg_pct_away: REAL\n",
      "   - fg3m_away: REAL\n",
      "   - fg3a_away: REAL\n",
      "   - fg3_pct_away: REAL\n",
      "   - ftm_away: REAL\n",
      "   - fta_away: REAL\n",
      "   - ft_pct_away: REAL\n",
      "   - oreb_away: REAL\n",
      "   - dreb_away: REAL\n",
      "   - reb_away: REAL\n",
      "   - ast_away: REAL\n",
      "   - stl_away: REAL\n",
      "   - blk_away: REAL\n",
      "   - tov_away: REAL\n",
      "   - pf_away: REAL\n",
      "   - pts_away: REAL\n",
      "   - plus_minus_away: INTEGER\n",
      "   - video_available_away: INTEGER\n",
      "   - season_type: TEXT\n",
      "\n",
      "Table: game_summary\n",
      "   - game_date_est: TIMESTAMP\n",
      "   - game_sequence: INTEGER\n",
      "   - game_id: TEXT\n",
      "   - game_status_id: INTEGER\n",
      "   - game_status_text: TEXT\n",
      "   - gamecode: TEXT\n",
      "   - home_team_id: TEXT\n",
      "   - visitor_team_id: TEXT\n",
      "   - season: TEXT\n",
      "   - live_period: INTEGER\n",
      "   - live_pc_time: TEXT\n",
      "   - natl_tv_broadcaster_abbreviation: TEXT\n",
      "   - live_period_time_bcast: TEXT\n",
      "   - wh_status: INTEGER\n",
      "\n",
      "Table: other_stats\n",
      "   - game_id: TEXT\n",
      "   - league_id: TEXT\n",
      "   - team_id_home: TEXT\n",
      "   - team_abbreviation_home: TEXT\n",
      "   - team_city_home: TEXT\n",
      "   - pts_paint_home: INTEGER\n",
      "   - pts_2nd_chance_home: INTEGER\n",
      "   - pts_fb_home: INTEGER\n",
      "   - largest_lead_home: INTEGER\n",
      "   - lead_changes: INTEGER\n",
      "   - times_tied: INTEGER\n",
      "   - team_turnovers_home: INTEGER\n",
      "   - total_turnovers_home: INTEGER\n",
      "   - team_rebounds_home: INTEGER\n",
      "   - pts_off_to_home: INTEGER\n",
      "   - team_id_away: TEXT\n",
      "   - team_abbreviation_away: TEXT\n",
      "   - team_city_away: TEXT\n",
      "   - pts_paint_away: INTEGER\n",
      "   - pts_2nd_chance_away: INTEGER\n",
      "   - pts_fb_away: INTEGER\n",
      "   - largest_lead_away: INTEGER\n",
      "   - team_turnovers_away: INTEGER\n",
      "   - total_turnovers_away: INTEGER\n",
      "   - team_rebounds_away: INTEGER\n",
      "   - pts_off_to_away: INTEGER\n",
      "\n",
      "Table: officials\n",
      "   - game_id: TEXT\n",
      "   - official_id: TEXT\n",
      "   - first_name: TEXT\n",
      "   - last_name: TEXT\n",
      "   - jersey_num: TEXT\n",
      "\n",
      "Table: inactive_players\n",
      "   - game_id: TEXT\n",
      "   - player_id: TEXT\n",
      "   - first_name: TEXT\n",
      "   - last_name: TEXT\n",
      "   - jersey_num: TEXT\n",
      "   - team_id: TEXT\n",
      "   - team_city: TEXT\n",
      "   - team_name: TEXT\n",
      "   - team_abbreviation: TEXT\n",
      "\n",
      "Table: game_info\n",
      "   - game_id: TEXT\n",
      "   - game_date: TIMESTAMP\n",
      "   - attendance: INTEGER\n",
      "   - game_time: TEXT\n",
      "\n",
      "Table: line_score\n",
      "   - game_date_est: TIMESTAMP\n",
      "   - game_sequence: INTEGER\n",
      "   - game_id: TEXT\n",
      "   - team_id_home: TEXT\n",
      "   - team_abbreviation_home: TEXT\n",
      "   - team_city_name_home: TEXT\n",
      "   - team_nickname_home: TEXT\n",
      "   - team_wins_losses_home: TEXT\n",
      "   - pts_qtr1_home: TEXT\n",
      "   - pts_qtr2_home: TEXT\n",
      "   - pts_qtr3_home: TEXT\n",
      "   - pts_qtr4_home: TEXT\n",
      "   - pts_ot1_home: INTEGER\n",
      "   - pts_ot2_home: INTEGER\n",
      "   - pts_ot3_home: INTEGER\n",
      "   - pts_ot4_home: INTEGER\n",
      "   - pts_ot5_home: INTEGER\n",
      "   - pts_ot6_home: INTEGER\n",
      "   - pts_ot7_home: INTEGER\n",
      "   - pts_ot8_home: INTEGER\n",
      "   - pts_ot9_home: INTEGER\n",
      "   - pts_ot10_home: INTEGER\n",
      "   - pts_home: REAL\n",
      "   - team_id_away: TEXT\n",
      "   - team_abbreviation_away: TEXT\n",
      "   - team_city_name_away: TEXT\n",
      "   - team_nickname_away: TEXT\n",
      "   - team_wins_losses_away: TEXT\n",
      "   - pts_qtr1_away: INTEGER\n",
      "   - pts_qtr2_away: TEXT\n",
      "   - pts_qtr3_away: TEXT\n",
      "   - pts_qtr4_away: INTEGER\n",
      "   - pts_ot1_away: INTEGER\n",
      "   - pts_ot2_away: INTEGER\n",
      "   - pts_ot3_away: INTEGER\n",
      "   - pts_ot4_away: INTEGER\n",
      "   - pts_ot5_away: INTEGER\n",
      "   - pts_ot6_away: INTEGER\n",
      "   - pts_ot7_away: INTEGER\n",
      "   - pts_ot8_away: INTEGER\n",
      "   - pts_ot9_away: INTEGER\n",
      "   - pts_ot10_away: INTEGER\n",
      "   - pts_away: REAL\n",
      "\n",
      "Table: player\n",
      "   - id: TEXT\n",
      "   - full_name: TEXT\n",
      "   - first_name: TEXT\n",
      "   - last_name: TEXT\n",
      "   - is_active: INTEGER\n",
      "\n",
      "Table: team\n",
      "   - id: TEXT\n",
      "   - full_name: TEXT\n",
      "   - abbreviation: TEXT\n",
      "   - nickname: TEXT\n",
      "   - city: TEXT\n",
      "   - state: TEXT\n",
      "   - year_founded: REAL\n",
      "\n",
      "Table: common_player_info\n",
      "   - person_id: TEXT\n",
      "   - first_name: TEXT\n",
      "   - last_name: TEXT\n",
      "   - display_first_last: TEXT\n",
      "   - display_last_comma_first: TEXT\n",
      "   - display_fi_last: TEXT\n",
      "   - player_slug: TEXT\n",
      "   - birthdate: TIMESTAMP\n",
      "   - school: TEXT\n",
      "   - country: TEXT\n",
      "   - last_affiliation: TEXT\n",
      "   - height: TEXT\n",
      "   - weight: TEXT\n",
      "   - season_exp: REAL\n",
      "   - jersey: TEXT\n",
      "   - position: TEXT\n",
      "   - rosterstatus: TEXT\n",
      "   - games_played_current_season_flag: TEXT\n",
      "   - team_id: INTEGER\n",
      "   - team_name: TEXT\n",
      "   - team_abbreviation: TEXT\n",
      "   - team_code: TEXT\n",
      "   - team_city: TEXT\n",
      "   - playercode: TEXT\n",
      "   - from_year: REAL\n",
      "   - to_year: REAL\n",
      "   - dleague_flag: TEXT\n",
      "   - nba_flag: TEXT\n",
      "   - games_played_flag: TEXT\n",
      "   - draft_year: TEXT\n",
      "   - draft_round: TEXT\n",
      "   - draft_number: TEXT\n",
      "   - greatest_75_flag: TEXT\n",
      "\n",
      "Table: team_details\n",
      "   - team_id: TEXT\n",
      "   - abbreviation: TEXT\n",
      "   - nickname: TEXT\n",
      "   - yearfounded: REAL\n",
      "   - city: TEXT\n",
      "   - arena: TEXT\n",
      "   - arenacapacity: REAL\n",
      "   - owner: TEXT\n",
      "   - generalmanager: TEXT\n",
      "   - headcoach: TEXT\n",
      "   - dleagueaffiliation: TEXT\n",
      "   - facebook: TEXT\n",
      "   - instagram: TEXT\n",
      "   - twitter: TEXT\n",
      "\n",
      "Table: team_history\n",
      "   - team_id: TEXT\n",
      "   - city: TEXT\n",
      "   - nickname: TEXT\n",
      "   - year_founded: INTEGER\n",
      "   - year_active_till: INTEGER\n",
      "\n",
      "Table: draft_combine_stats\n",
      "   - season: TEXT\n",
      "   - player_id: TEXT\n",
      "   - first_name: TEXT\n",
      "   - last_name: TEXT\n",
      "   - player_name: TEXT\n",
      "   - position: TEXT\n",
      "   - height_wo_shoes: REAL\n",
      "   - height_wo_shoes_ft_in: TEXT\n",
      "   - height_w_shoes: REAL\n",
      "   - height_w_shoes_ft_in: TEXT\n",
      "   - weight: TEXT\n",
      "   - wingspan: REAL\n",
      "   - wingspan_ft_in: TEXT\n",
      "   - standing_reach: REAL\n",
      "   - standing_reach_ft_in: TEXT\n",
      "   - body_fat_pct: TEXT\n",
      "   - hand_length: TEXT\n",
      "   - hand_width: TEXT\n",
      "   - standing_vertical_leap: REAL\n",
      "   - max_vertical_leap: REAL\n",
      "   - lane_agility_time: REAL\n",
      "   - modified_lane_agility_time: REAL\n",
      "   - three_quarter_sprint: REAL\n",
      "   - bench_press: REAL\n",
      "   - spot_fifteen_corner_left: TEXT\n",
      "   - spot_fifteen_break_left: TEXT\n",
      "   - spot_fifteen_top_key: TEXT\n",
      "   - spot_fifteen_break_right: TEXT\n",
      "   - spot_fifteen_corner_right: TEXT\n",
      "   - spot_college_corner_left: TEXT\n",
      "   - spot_college_break_left: TEXT\n",
      "   - spot_college_top_key: TEXT\n",
      "   - spot_college_break_right: TEXT\n",
      "   - spot_college_corner_right: TEXT\n",
      "   - spot_nba_corner_left: TEXT\n",
      "   - spot_nba_break_left: TEXT\n",
      "   - spot_nba_top_key: TEXT\n",
      "   - spot_nba_break_right: TEXT\n",
      "   - spot_nba_corner_right: TEXT\n",
      "   - off_drib_fifteen_break_left: TEXT\n",
      "   - off_drib_fifteen_top_key: TEXT\n",
      "   - off_drib_fifteen_break_right: TEXT\n",
      "   - off_drib_college_break_left: TEXT\n",
      "   - off_drib_college_top_key: TEXT\n",
      "   - off_drib_college_break_right: TEXT\n",
      "   - on_move_fifteen: TEXT\n",
      "   - on_move_college: TEXT\n",
      "\n",
      "Table: draft_history\n",
      "   - person_id: TEXT\n",
      "   - player_name: TEXT\n",
      "   - season: TEXT\n",
      "   - round_number: INTEGER\n",
      "   - round_pick: INTEGER\n",
      "   - overall_pick: INTEGER\n",
      "   - draft_type: TEXT\n",
      "   - team_id: TEXT\n",
      "   - team_city: TEXT\n",
      "   - team_name: TEXT\n",
      "   - team_abbreviation: TEXT\n",
      "   - organization: TEXT\n",
      "   - organization_type: TEXT\n",
      "   - player_profile_flag: TEXT\n",
      "\n",
      "Table: team_info_common\n",
      "   - team_id: TEXT\n",
      "   - season_year: TEXT\n",
      "   - team_city: TEXT\n",
      "   - team_name: TEXT\n",
      "   - team_abbreviation: TEXT\n",
      "   - team_conference: TEXT\n",
      "   - team_division: TEXT\n",
      "   - team_code: TEXT\n",
      "   - team_slug: TEXT\n",
      "   - w: INTEGER\n",
      "   - l: INTEGER\n",
      "   - pct: REAL\n",
      "   - conf_rank: INTEGER\n",
      "   - div_rank: INTEGER\n",
      "   - min_year: INTEGER\n",
      "   - max_year: INTEGER\n",
      "   - league_id: TEXT\n",
      "   - season_id: TEXT\n",
      "   - pts_rank: INTEGER\n",
      "   - pts_pg: REAL\n",
      "   - reb_rank: INTEGER\n",
      "   - reb_pg: REAL\n",
      "   - ast_rank: INTEGER\n",
      "   - ast_pg: REAL\n",
      "   - opp_pts_rank: INTEGER\n",
      "   - opp_pts_pg: REAL\n"
     ]
    }
   ],
   "source": [
    "for table in tables:\n",
    "    columns = get_columns_for_table(table, db_path)\n",
    "    print(f\"\\nTable: {table}\")\n",
    "    for col_name, col_type in columns:\n",
    "        print(f\"   - {col_name}: {col_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9749e885",
   "metadata": {},
   "source": [
    "#### Engineering the prompts based on above recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b407da02",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_level_prompt=\"\"\" \n",
    "<instruction>\n",
    "You are an expert data analyst that converts natural language questions into SQL on a National Basketball Association Database\n",
    "</instruction> \n",
    "\n",
    "<instruction>\n",
    "You only have access to a helper function defined below and no other external resources.\n",
    "\n",
    "<helper_function>\n",
    "def describe_table(table_name):\n",
    "    return f\"Returns the schema of the specified table from the NBA database, showing column names and types.\"\n",
    "</helper_function>\n",
    "\n",
    "</instruction>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "2b1f3a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_prompt=\"\"\"\n",
    "Convert the following question into a SQL query:\n",
    "\n",
    "<question>{question}</question>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "2f27883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_type=\"\"\"\n",
    "This Natural Language question if of the following type:\n",
    "\n",
    "<query_type>{query_type}</query_type>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64413594",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_db_schema=\"\"\"\n",
    "For the purpose of this task, refer to the following schema of tables, the column names are included in the description:\n",
    "\n",
    "<tables_list>\n",
    "    <table>\n",
    "        <table_name>team_info_common</table_name>\n",
    "        <description>This table has details about individual teams like team_id, team_city, team_name</description>\n",
    "    </table>\n",
    "        <table_name>team_history</table_name> \n",
    "        <description>Refer to this table for details and has column names like year_founded, year_active_till</description>\n",
    "    </table>\n",
    "        <table_name>common_player_info</table_name>\n",
    "        <description>This table has commonly asked details about individual players like person_id, team_name</description>\n",
    "    </table>\n",
    "        <table_name>game_info</table_name>\n",
    "        <description>This table has commonly asked details about games like game_id, game_date</description>\n",
    "    </table>\n",
    "        <table_name>inactive_players</table_name>\n",
    "        <description>This table has commonly asked details about inactive players like player_id</description>\n",
    "    </table>\n",
    "</tables_list>\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cabb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "precognition=\"\"\"\n",
    "Before you start writing the SQL, think about the question. Put your thinking into \"\" tags and return it in your response. \n",
    "\n",
    "When you are thinking, consider these points in the tags:\n",
    "\n",
    "<instructions>\n",
    "Refer to the schema to find which tables you need to query. If you do not find a table name, think about what the table name could be.\n",
    "</instructions>\n",
    "\n",
    "<instructions>\n",
    "Once you have the table name, use the helper function to get column names.\n",
    "</instructions>\n",
    "\n",
    "<instructions>\n",
    "Based on the query type provided above, think about the kind of output in the SELECT statement - single columns or multiple columns.\n",
    "</instructions>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6c90c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_examples=\"\"\"\n",
    "Here are some natural language to query examples you can refer to:\n",
    "\n",
    "<example>\n",
    "<question>What are the top 5 teams with the most championships?</question>\n",
    "Thinking:\n",
    "- I need to check the schema for a table that contains a column about championships.\n",
    "- I use describe_table(\"team\").\n",
    "- Let's say it returns: id (int), full_name (test), championships_won (int)\n",
    "- Now I can sort by championships_won and limit to 5.\n",
    "\n",
    "<sql>\n",
    "SELECT full_name FROM team ORDER BY championships_won DESC LIMIT 5;\n",
    "</sql>\n",
    "</example>\n",
    "\n",
    "<example>\n",
    "<question> List all players who are currently active and taller than 7 feet.</question>\n",
    "<thinking>\n",
    "- Use describe_table(\"common_player_info\") to inspect columns.\n",
    "- Assume it includes: id, name, height_in_inches, is_active\n",
    "- Filter players where height > 84 inches and is_active = 1.\n",
    "</thinking>\n",
    "\n",
    "<sql>:\n",
    "SELECT name FROM common_player_info WHERE height_in_inches > 84 AND is_active = 1;\n",
    "</sql>\n",
    "</example>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95740a90",
   "metadata": {},
   "source": [
    "#### We combine these prompts in a sequence that gives Claude enough sequential context. Here is one possible sequence:\n",
    "- system_level_prompt: so that Claude knows its roles and constraints\n",
    "- few_shot_db_schema: gives Claude a high level map of available tables/columns so that subsequent examples make sense\n",
    "- few_shot_examples: Shows correct thinking patterns for Claude.\n",
    "- precognition: gives step by step reasoning instructions before generating a SQL query\n",
    "- query_type: This gives more context about the type of query\n",
    "- main_prompt: This is the NL->SQL question.\n",
    "\n",
    "\n",
    "- We will build the main prompt by concatenating the above sub-prompts. The main prompt is created in a way such that we can remove subprompts while prompt engineering without affecting the rest of the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda3e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_prompt=\"\"\n",
    "if system_level_prompt:\n",
    "    main_prompt+=f\"\"\"{system_level_prompt}\"\"\"\n",
    "if few_shot_db_schema:\n",
    "    main_prompt+=f\"\"\"{few_shot_db_schema}\"\"\"\n",
    "if few_shot_examples:\n",
    "    main_prompt+=f\"\"\"{few_shot_examples}\"\"\"\n",
    "if precognition:\n",
    "    main_prompt+=f\"\"\"{precognition}\"\"\"\n",
    "if query_type:\n",
    "    main_prompt+=f\"\"\"{precognition}\"\"\"\n",
    "main_prompt+=question_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05504dba",
   "metadata": {},
   "source": [
    "#### We can call our function prompt_engineering_result which runs everything and gives us an evaluation of the final results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff5853e",
   "metadata": {},
   "source": [
    "#### Trial 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "90c30756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"question\": \"What's the average weight of power forwards?\",\n",
      "  \"ground_truth_sql\": \"SELECT ROUND(AVG(CAST(weight AS FLOAT)), 2) as avg_weight FROM common_player_info WHERE position LIKE '%F%' AND weight != ''\",\n",
      "  \"gorund_truth_query_type\": \"aggregation\",\n",
      "  \"claude_sql\": \"SELECT AVG(weight) AS average_weight\\nFROM common_player_info\\nWHERE position = 'PF';\",\n",
      "  \"sql_semantic_match\": false,\n",
      "  \"ground_truth\": [\n",
      "    220.04\n",
      "  ],\n",
      "  \"claude_sql_result\": [\n",
      "    null\n",
      "  ]\n",
      "}\n",
      "Overall accuracy: 0.0 %\n",
      "{\n",
      "  \"question\": \"Which team has the newest arena?\",\n",
      "  \"ground_truth_sql\": \"SELECT t.full_name FROM team t JOIN team_details td ON t.id = td.team_id ORDER BY td.arena DESC LIMIT 1\",\n",
      "  \"gorund_truth_query_type\": \"detail\",\n",
      "  \"claude_sql\": \"SELECT t.team_name, t.team_city, a.arena_name, a.year_built\\nFROM team_info_common t\\nJOIN team_arenas a ON t.team_id = a.team_id\\nORDER BY a.year_built DESC\\nLIMIT 1;\",\n",
      "  \"sql_semantic_match\": false,\n",
      "  \"ground_truth\": [\n",
      "    \"Philadelphia 76ers\"\n",
      "  ],\n",
      "  \"claude_sql_result\": null\n",
      "}\n",
      "Overall accuracy: 0.0 %\n"
     ]
    }
   ],
   "source": [
    "results,claude_sql_raw=prompt_engineering_result(main_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e542e6",
   "metadata": {},
   "source": [
    "#### Checkout the claude_sql_raw to see how Claude was thinking\n",
    "- While the accuracy still remains poor, we can also see that Claude was able to query correct table names in some cases.\n",
    "- We see that letting Claude think gives it better direction to build a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "943f14c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll convert this natural language question into a SQL query.\n",
      "\n",
      "\"To answer this question about the average weight of power forwards, I need to:\n",
      "\n",
      "1. Find a table that contains player positions and weights.\n",
      "2. From the tables list, 'common_player_info' seems most relevant as it has 'commonly asked details about individual players'.\n",
      "3. I would need to use the describe_table helper function to see if this table has columns for position and weight.\n",
      "4. Since I don't have the actual ability to call the helper function, I'll assume common_player_info has columns like:\n",
      "   - person_id\n",
      "   - player_name\n",
      "   - position\n",
      "   - weight\n",
      "5. I need to filter for players whose position is 'PF' (Power Forward) and calculate the average weight.\n",
      "6. The output will be a single value - the average weight of power forwards.\"\n",
      "\n",
      "```sql\n",
      "SELECT AVG(weight) AS average_weight\n",
      "FROM common_player_info\n",
      "WHERE position = 'PF';\n",
      "``` \n",
      " ############################################################ \n",
      "\n",
      "I'll convert this natural language question into a SQL query.\n",
      "\n",
      "\"To find which team has the newest arena, I need to:\n",
      "1. Look for a table that contains information about team arenas\n",
      "2. From the tables_list, I don't see a specific table for arenas\n",
      "3. The team_info_common table might have arena information since it contains details about teams\n",
      "4. I would use describe_table('team_info_common') to see if it has arena-related columns\n",
      "5. If team_info_common doesn't have arena information, there might be another table not listed that contains arena details\n",
      "6. Without seeing the exact schema, I'll assume there's a table called 'team_arenas' or that arena_year_built is in team_info_common\n",
      "7. I need to order by the arena construction year in descending order and limit to 1 to get the newest arena\"\n",
      "\n",
      "Based on the available information, I'll write a SQL query assuming there's either a column in team_info_common or a separate table for arenas:\n",
      "\n",
      "```sql\n",
      "SELECT t.team_name, t.team_city, a.arena_name, a.year_built\n",
      "FROM team_info_common t\n",
      "JOIN team_arenas a ON t.team_id = a.team_id\n",
      "ORDER BY a.year_built DESC\n",
      "LIMIT 1;\n",
      "```\n",
      "\n",
      "Note: This query assumes there's a table called 'team_arenas' with columns for arena_name and year_built. If the actual schema is different, the query would need to be adjusted based on the actual table and column names. \n",
      " ############################################################ \n",
      "\n",
      "\"To answer this question about the total number of games played, I need to:\n",
      "\n",
      "1. Find a table that contains game information\n",
      "2. Looking at the tables_list, I see there's a 'game_info' table which would likely contain records of all games\n",
      "3. I need to count the total number of records in this table, as each record would represent a game\n",
      "4. I would use the COUNT() function to get this total\n",
      "5. Since I'm just getting a count of all games, I don't need any filtering conditions\n",
      "\n",
      "I would use the helper function describe_table('game_info') to get the exact schema, but based on the information provided, I can write a query that counts all records in the game_info table.\"\n",
      "\n",
      "```sql\n",
      "SELECT COUNT(*) AS total_games\n",
      "FROM game_info;\n",
      "```\n",
      "\n",
      "This query will return a single value representing the total number of games in the database by counting all rows in the game_info table. \n",
      " ############################################################ \n",
      "\n",
      "I'll convert this natural language question into a SQL query.\n",
      "\n",
      "\"To answer this question about which team has the highest percentage of games with 10+ blocks, I need to:\n",
      "\n",
      "1. Find tables related to teams, games, and player statistics.\n",
      "2. From the provided schema, I see 'team_info_common' for team information and 'game_info' for game details.\n",
      "3. However, I don't see a specific table for game statistics like blocks. I would need a table that tracks team statistics per game.\n",
      "4. I'll assume there might be a table called 'team_game_stats' or similar that contains block statistics.\n",
      "5. I need to:\n",
      "   - Count games where a team had 10+ blocks\n",
      "   - Count total games played by each team\n",
      "   - Calculate the percentage\n",
      "   - Order by this percentage to find the highest\n",
      "\n",
      "Since I don't have the exact table structure for game statistics, I'll make reasonable assumptions about table and column names.\"\n",
      "\n",
      "```sql\n",
      "-- Assuming there's a table called team_game_stats with columns:\n",
      "-- team_id, game_id, blocks, and other statistics\n",
      "\n",
      "SELECT \n",
      "    t.team_name,\n",
      "    COUNT(CASE WHEN tgs.blocks >= 10 THEN 1 END) * 100.0 / COUNT(*) AS block_percentage\n",
      "FROM \n",
      "    team_game_stats tgs\n",
      "JOIN \n",
      "    team_info_common t ON tgs.team_id = t.team_id\n",
      "GROUP BY \n",
      "    t.team_id, t.team_name\n",
      "ORDER BY \n",
      "    block_percentage DESC\n",
      "LIMIT 1;\n",
      "```\n",
      "\n",
      "Note: This query assumes the existence of a 'team_game_stats' table that contains block statistics per game. If the actual table structure is different, the query would need to be adjusted accordingly. \n",
      " ############################################################ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in claude_sql_raw[0:4]:\n",
    "    print(i,\"\\n\",\"#\"*60,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c2b3bd",
   "metadata": {},
   "source": [
    "#### Testing out our new prompt on different types of queries\n",
    "- Now we try running the above engineered prompt on just counting query types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "d46f85c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "counting_query_type_data = [i for i in ground_truth_data if i.get(\"type\") == \"counting\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "031cf266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"question\": \"How many teams are in the NBA?\",\n",
      "  \"ground_truth_sql\": \"SELECT COUNT(*) as team_count FROM team LIMIT 1\",\n",
      "  \"gorund_truth_query_type\": \"counting\",\n",
      "  \"claude_sql\": \"SELECT COUNT(*) AS total_teams\\nFROM team_info_common;\",\n",
      "  \"sql_semantic_match\": false,\n",
      "  \"ground_truth\": [\n",
      "    30\n",
      "  ],\n",
      "  \"claude_sql_result\": [\n",
      "    0\n",
      "  ]\n",
      "}\n",
      "Overall accuracy: 15.79 %\n",
      "{\n",
      "  \"question\": \"What's the total number of games played?\",\n",
      "  \"ground_truth_sql\": \"SELECT COUNT(DISTINCT game_id) as total_games FROM game LIMIT 1\",\n",
      "  \"gorund_truth_query_type\": \"counting\",\n",
      "  \"claude_sql\": \"SELECT COUNT(*) AS total_games\\nFROM game_info;\",\n",
      "  \"sql_semantic_match\": false,\n",
      "  \"ground_truth\": [\n",
      "    65642\n",
      "  ],\n",
      "  \"claude_sql_result\": [\n",
      "    58053\n",
      "  ]\n",
      "}\n",
      "Overall accuracy: 15.79 %\n"
     ]
    }
   ],
   "source": [
    "results_trial_2,claude_sql_raw_trial_2=prompt_engineering_result(main_prompt,counting_query_type_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "57f9ad46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.79"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(results_trial_2['accuracy']*100,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a601e166",
   "metadata": {},
   "source": [
    "- We see a ~16% lift in accuracy for Counting based queries in Trial 2 using our prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7df124",
   "metadata": {},
   "source": [
    "**Next steps**\n",
    "- We try out aggregation type queries now\n",
    "- Based on this lift, it is possible that more advanced queries generated by Claude like filtering, ranking (especially because they could require joins) are not performing as well on the DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "3d588a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"question\": \"What's the average points per game?\",\n",
      "  \"ground_truth_sql\": \"SELECT ROUND(AVG(pts_home + pts_away) / 2, 2) as avg_points FROM game LIMIT 1\",\n",
      "  \"gorund_truth_query_type\": \"aggregation\",\n",
      "  \"claude_sql\": \"SELECT AVG(points) AS average_points_per_game\\nFROM game_stats;\",\n",
      "  \"sql_semantic_match\": false,\n",
      "  \"ground_truth\": [\n",
      "    102.81\n",
      "  ],\n",
      "  \"claude_sql_result\": null\n",
      "}\n",
      "Overall accuracy: 0.0 %\n",
      "{\n",
      "  \"question\": \"What's the average height of NBA players?\",\n",
      "  \"ground_truth_sql\": \"SELECT ROUND(AVG(CAST(SUBSTR(height, 1, INSTR(height, '-')-1) AS FLOAT)), 2) as avg_height FROM common_player_info WHERE height != '' LIMIT 1\",\n",
      "  \"gorund_truth_query_type\": \"aggregation\",\n",
      "  \"claude_sql\": \"SELECT AVG(height) AS average_height\\nFROM common_player_info;\",\n",
      "  \"sql_semantic_match\": false,\n",
      "  \"ground_truth\": [\n",
      "    6.03\n",
      "  ],\n",
      "  \"claude_sql_result\": [\n",
      "    5.910242290748899\n",
      "  ]\n",
      "}\n",
      "Overall accuracy: 0.0 %\n"
     ]
    }
   ],
   "source": [
    "aggregation_query_type_data = [i for i in ground_truth_data if i.get(\"type\") == \"aggregation\"]\n",
    "results_trial_3,claude_sql_raw_trial_3=prompt_engineering_result(main_prompt,aggregation_query_type_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597ed7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d2ea8cf",
   "metadata": {},
   "source": [
    "- In the above results for avg weight, we see that Claude was close to actual queries, and we can add additional metrics like rounding in aggregration based queries in our precognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0294c94",
   "metadata": {},
   "source": [
    "### Trial 2\n",
    "- We also add thinking for primary-keys that could be foreign-key pairs for queries requiring joins work, we give a few shot example for join. We keep just two examples to not make the full prompt too verbose. However, keeping the earlier 2 and adding a new one could also change our results. Feel free to try it out!\n",
    "- We also add rounding elements for aggregation type queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd20ba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "precognition=\"\"\"\n",
    "Before you start writing the SQL, think about the question. Put your thinking into \"\" tags and return it in your response. \n",
    "\n",
    "When you are thinking, consider these points in the tags:\n",
    "\n",
    "<instructions>\n",
    "Refer to the schema to find which tables you need to query. If you do not find a table name, think about what the table name could be.\n",
    "</instructions>\n",
    "\n",
    "<instructions>\n",
    "Once you have the table name, use the helper function to get column names.\n",
    "</instructions>\n",
    "\n",
    "<instructions>\n",
    "Think about tables that might need to be joined \n",
    "</instructions>\n",
    "\n",
    "<instructions>\n",
    "Based on the query type provided above, think about the kind of output in the SELECT statement - single columns or multiple columns.\n",
    "</instructions>\n",
    "\n",
    "<instructions>\n",
    "If the query type is <query_type>aggregation</query_type> then round your outputs to two decimal places.\n",
    "</instructions>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "f3175810",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_examples=\"\"\"\n",
    "Here are some natural language to query examples you can refer to:\n",
    "\n",
    "<example>\n",
    "<question>What are the top 5 teams with the most championships?</question>\n",
    "Thinking:\n",
    "- I need to check the schema for a table that contains a column about championships.\n",
    "- I use describe_table(\"team\").\n",
    "- Let's say it returns: id (int), full_name (test), championships_won (int)\n",
    "- Now I can sort by championships_won and limit to 5.\n",
    "\n",
    "<sql>\n",
    "SELECT full_name FROM team ORDER BY championships_won DESC LIMIT 5;\n",
    "</sql>\n",
    "</example>\n",
    "\n",
    "<example>\n",
    "<question>List all players and their teams for the 2022 season.</question>\n",
    "<thinking>\n",
    "- Use describe_table(\"common_player_info\") and describe_table(\"team\") to inspect structure.\n",
    "- Assume common_player_info includes: player_id, season, team_id\n",
    "- Assume team includes: id, full_name\n",
    "- Join on team_id = team.id and filter by season = 2022.\n",
    "</thinking>\n",
    "<sql>\n",
    "SELECT p.player_id, t.full_name \n",
    "FROM player_season_stats p\n",
    "JOIN team t ON p.team_id = t.id\n",
    "WHERE p.season = 2022;\n",
    "</sql>\n",
    "</example>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "e4ba61fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_prompt=\"\"\n",
    "if system_level_prompt:\n",
    "    main_prompt+=f\"\"\"{system_level_prompt}\"\"\"\n",
    "if few_shot_db_schema:\n",
    "    main_prompt+=f\"\"\"{few_shot_db_schema}\"\"\"\n",
    "if few_shot_examples:\n",
    "    main_prompt+=f\"\"\"{few_shot_examples}\"\"\"\n",
    "if precognition:\n",
    "    main_prompt+=f\"\"\"{precognition}\"\"\"\n",
    "if query_type:\n",
    "    main_prompt+=f\"\"\"{precognition}\"\"\"\n",
    "main_prompt+=question_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f594e0",
   "metadata": {},
   "source": [
    "\n",
    "We run the same on aggregation query type to see if there is any lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b85aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_trial_4,claude_sql_raw_trial_4=prompt_engineering_result(main_prompt,aggregation_query_type_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382536c2",
   "metadata": {},
   "source": [
    "#### Next steps:\n",
    "\n",
    "- Your analytics team can try out this setup and engineer prompts to achieve accuracy lifts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9701b07",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
